{"cells":[{"cell_type":"markdown","source":["# Function Notebook\n\nIn this notebook we define all of our project functions, relevants imports and Global Parameters. In each notebook with import the functions using 'libify' library."],"metadata":{}},{"cell_type":"code","source":["### Imports ###\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nimport pickle\nfrom pyspark.sql import SQLContext\nfrom scipy.stats import beta \nfrom sklearn import preprocessing\nimport os\n\nos.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.elasticsearch:elasticsearch-hadoop:7.9.3 pyspark-shell'\n\n!python -m pip install elasticsearch\nfrom elasticsearch import Elasticsearch\n\nfrom pyspark.sql.functions import date_format\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport pandas as pd # for small table fancy display only\n\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoderEstimator, Normalizer, StringIndexerModel, OneHotEncoderModel, VectorSizeHint\nfrom pyspark.ml.regression import LinearRegression, RandomForestRegressor\nfrom pyspark.ml.linalg import SparseVector, DenseVector,Vectors, VectorUDT\nfrom pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nfrom pyspark.ml.evaluation import RegressionEvaluator,  BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n\nfrom IPython.display import Image\nfrom IPython.core.display import HTML\n\nimport matplotlib.pyplot as plt \nimport numpy as np\n\nimport sys\n\nimport random\nimport string"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["## Global Parameters ##\nelastic_settings_mappings_basic = {\n    \"settings\": {\n        \"number_of_shards\": 1,\n        \"number_of_replicas\": 0,\n        \"refresh_interval\" : -1\n    }\n}\n\nelastic_settings_mappings_centorids = {\n    \"settings\": {\n        \"number_of_shards\": 1,\n        \"number_of_replicas\": 0,\n        \"refresh_interval\" : -1\n    },\n    \"mappings\": {\n        \"properties\": {\n            \"centroids\" : { \"type\": \"geo_point\" }\n        }\n    }\n}\n\nelastic_settings_mappings_date = {\n    \"settings\": {\n        \"number_of_shards\": 1,\n        \"number_of_replicas\": 0,\n        \"refresh_interval\" : -1\n    },\n    \"mappings\": {\n        \"properties\": {\n            \"date\" : { \"type\": \"date\"},\n        }\n    }\n}\n\n# ES_HOST = 'da2020w-0001.eastus.cloudapp.azure.com'\nES_HOST = '10.0.0.5'\nes = Elasticsearch([{'host': ES_HOST}], timeout=600000)\n\nbase_path = \"/dbfs/FileStore/shared_uploads/shubi@campus.technion.ac.il/\""],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["### Helper Functions ###\ndef create_warehouse_df(streaming=False, USE_TAKE=False, min_date = None, max_date = None, path_ip=None):\n    ### Schema ###\n    schema = pickle.load(open(\"/dbfs/mnt/schema.pkl\", \"rb\"))\n    try:\n        if not streaming:\n            ### Load and Transfrom df ###\n            df = spark.read.json(path_ip, schema=schema)\n\n            if USE_TAKE:\n                df = spark.createDataFrame(df.take(100000)).cache()\n\n        else:  # Means we are Streaming\n            \"\"\"\n                Here we read the stream from Kafkfa. \n                We specifiy the IP to read from, \n                the Topics to subscribe to (by Vehicle ID)\n                the offset - which means from what point in time to take the data - in our case 'earliest', which means all\n            \"\"\"\n            # Subscribe to multiple topics\n            if USE_TAKE: # # 28051 = 23,057 records, vehicle_id - 28052 - 14,692\n                kafka_raw_df = spark \\\n                  .readStream \\\n                  .format(\"kafka\") \\\n                  .option(\"kafka.bootstrap.servers\", path_ip) \\\n                  .option(\"subscribe\", \"vehicleId_28051, vehicleId_28052, vehicleId_33317, vehicleId_33318\") \\\n                  .option(\"startingOffsets\", \"earliest\") \\\n                  .load()\n\n            # Subscribe to a pattern\n            else:\n                kafka_raw_df = spark \\\n                  .readStream \\\n                  .format(\"kafka\") \\\n                  .option(\"kafka.bootstrap.servers\", path_ip) \\\n                  .option(\"subscribePattern\", \"vehicleId_.*\") \\\n                  .option(\"startingOffsets\", \"earliest\") \\\n                  .load()\n\n            df = kafka_raw_df.selectExpr(\"CAST(value AS STRING)\") \\\n                       .select(F.from_json(F.col(\"value\"), schema=schema).alias('json')) \\\n                       .select(\"json.*\")\n    except Exception:\n        return \"problem\"\n    \n    df = df.withColumn('timestamp', df['timestamp']['$numberLong'].cast('bigint'))\\\n         .withColumn('calendar', df['calendar']['$numberLong'].cast('bigint'))\n    \n\n    df = df.withColumn('date', F.from_unixtime((df['timestamp'])/1000, 'yyyy-MM-dd HH:mm:ss').cast('timestamp'))\\\n         .withColumn('loc', df['loc']['coordinates'])\\\n         .drop(\"timestamp\")\\\n         .withColumn('calendar', F.from_unixtime((df['calendar'])/1000000, 'yyyy-MM-dd HH:mm:ss').cast('timestamp'))\\\n         .withColumn('systemTimestamp', F.to_timestamp(df['systemTimestamp']))\n    \n    if USE_TAKE and min_date and max_date:\n        df = df.where(df.date.between(min_date, max_date))\n\n    df = df.drop(\"_id\",\"calendar\", \"systemTimestamp\", \"probability\", \"poiId\", \"poiId2\", \"longitude\", \"latitude\", \"direction\", \"dateTypeEnum\", \"dateType\", \"currentHour\", \"angle\", \"filteredActualDelay\", 'anomaly', 'gridID', 'actualDelay', 'delay', 'justLeftStop', 'vehicleSpeed')\n\n    df = df.withColumn(\"is_weekend\", date_format(\"date\", 'EEE').isin([\"Sat\", \"Sun\"]).cast(\"int\"))\n\n    df = df.withColumn(\"currentHour\", F.hour(\"date\"))\n\n    df = df.withColumn('ellapsedTime',df['ellapsedTime']/1000)\n\n    df = df.drop('lineId')\n    \n#     df = df.dropDuplicates()\n\n    return df\n\n@F.udf\ndef hour_group(hour):\n    if hour < 4:\n        return int(0) # '0-3'\n    elif hour < 8:\n        return int(1) # '4-7'\n    elif hour < 12:\n        return int(2) # '8-11'\n    elif hour < 16:\n        return int(3) #'12-15'\n    elif hour < 20:\n        return int(4) #'16-19'\n    else:\n        return int(5) #'20-23'\n      \n@F.udf\ndef number_to_hour_group(hour):\n    return {0:'0-3', 1:'4-7', 2:'8-11',3:'12-15',4:'16-19',5:'20-23'}[hour]\n\n\ndef column_to_index_with_pre_index(df, indexer_path):\n    indexer = StringIndexerModel.load(indexer_path)\n    df_indexed = indexer.transform(df)\n    return df_indexed\n\ndef column_to_ine_hot_with_pre_encoder(df_indexed, encoder_path):\n    encoder = OneHotEncoderModel.load(encoder_path)\n    df_encoded = encoder.transform(df_indexed)\n    return df_encoded\n\ndef sparse_to_array(v):\n    v = DenseVector(v)\n    new_array = list([float(x) for x in v])\n    return new_array\n\nsparse_to_array_udf = F.udf(sparse_to_array, ArrayType(FloatType()))\n\nto_vector = udf(lambda a: Vectors.dense(a), VectorUDT())\n\ndef read_elastic(index, query=\"\", scroll_size=\"10000\", array_field=\"\"):\n    if not es.indices.exists(index):\n        raise Exception(\"Index doesn't exist\")\n\n    return spark.read\\\n                .format(\"org.elasticsearch.spark.sql\")\\\n                .option(\"es.nodes.wan.only\",\"true\")\\\n                .option(\"es.port\",\"9200\")\\\n                .option(\"es.nodes\",ES_HOST)\\\n                .option(\"es.nodes.client.only\", \"false\")\\\n                .option(\"pushdown\", \"true\")\\\n                .option(\"es.query\", query)\\\n                .option(\"es.scroll.size\", scroll_size)\\\n                .option(\"es.scroll.keepalive\", \"120m\")\\\n                .option(\"es.read.field.as.array.include\", array_field)\\\n                .load(index)\n\ndef get_random_string(length):\n    letters = string.ascii_lowercase\n    result_str = ''.join(random.choice(letters) for i in range(length))\n    return result_str\n\ndef write_to_elastic(df_to_upload, index_name: str, settings, overwrite=False, streaming=True):    \n    if not es.indices.exists(index_name):\n        es.indices.create(index=index_name, ignore=400, body=settings)        \n    elif overwrite:\n        es.indices.delete(index=index_name)\n        es.indices.create(index=index_name, ignore=400, body=settings)\n    else:\n        print(\"Index already exists and overwrite flag is false\")\n        return\n        \n    if streaming:\n        df_to_upload.writeStream.format(\"org.elasticsearch.spark.sql\")\\\n        .option(\"es.resource\", index_name)\\\n        .option(\"es.nodes.wan.only\",\"true\")\\\n        .queryName(index_name)\\\n        .option(\"es.port\",\"9200\")\\\n        .option(\"es.nodes\",ES_HOST)\\\n        .option(\"es.nodes.client.only\", \"false\")\\\n        .outputMode('append')\\\n        .option(\"checkpointLocation\", \"/home/vmadmin/stream\"+get_random_string(8))\\\n        .start() \n    \n    else:\n        df_to_upload.write.format(\"org.elasticsearch.spark.sql\")\\\n            .option(\"es.resource\", index_name)\\\n            .option(\"es.nodes.wan.only\",\"true\")\\\n            .option(\"es.port\",\"9200\")\\\n            .option(\"es.nodes\",ES_HOST)\\\n            .option(\"es.nodes.client.only\", \"false\")\\\n            .save()\n\n \n\n@F.udf\ndef checkLocation(areaId, locationList):\n    if areaId in locationList:\n        return 1\n    else:\n        return 0\n  \n@F.udf\ndef checkDate(date, event_dates_list):\n    if date in event_dates_list:\n        return 1\n    else:\n        return 0  \n\ndef KNN(XTrain, yTrain, XTest):\n    knn = KNeighborsClassifier(n_neighbors=1)\n    knn.fit(XTrain, yTrain)\n    predictions = knn.predict(XTest)\n    return predictions\n\ndef recreate_locations_centroids():\n    df_centroids = read_elastic('areasidscentroids')\n    locations_df = read_elastic('locations')\n    \n    locationList = list(locations_df.select(\"areaId1\").distinct().rdd.flatMap(lambda x: x).collect())\n    \n    XTrain = df_centroids.select(\"lon_mean\", \"lat_mean\").toPandas()\n    yTrain = df_centroids.select(\"areaId1\").toPandas()\n    XTest = locations_df.select('lon','lat').toPandas()\n    \n    yPred = KNN(XTrain, yTrain, XTest)\n    \n    locations_centroids = locations_df.withColumn(\"centroids\", F.array(F.col(\"lon\"), F.col(\"lat\")))\n    locationNames = list(locations_centroids.select(\"locationName\").rdd.flatMap(lambda x: x).collect())\n    centroids = list(locations_centroids.select(\"centroids\").rdd.flatMap(lambda x: x).collect())\n    \n    locations_centroids = spark.createDataFrame([[loc, int(area), cent] for loc, area, cent in zip(locationNames, yPred, centroids)], schema=[\"locationNames\", \"areaId1\", \"centroids\"])\n    \n    return locations_centroids\n    \ndef hydrate_data(data):\n    \n    # Load from Elastic - locations\n    locations_df = read_elastic('locations')\n    locationList = list(locations_df.select(\"areaId1\").distinct().rdd.flatMap(lambda x: x).collect())\n    \n    # Load from Elastic - events\n    event_dates = read_elastic('events').withColumn(\"date\", F.to_date(F.col(\"date\")).cast(\"string\"))\n    event_dates_list = list(event_dates.select(\"date\").rdd.flatMap(lambda x: x).collect())\n    \n    df_hydrated = data.withColumn(\"eventsList\", F.array([F.lit(i) for i in event_dates_list]))\\\n                .withColumn('day', date_format(\"date\", 'yyyy-MM-dd'))\\\n                .withColumn('TimestampDay', F.to_timestamp(F.col('day')))\\\n                .withColumn(\"eventDate\", checkDate(F.col('day'), F.col('eventsList')))\\\n                .withColumn(\"locationList\", F.array([F.lit(int(i)) for i in locationList]))\\\n                .withColumn(\"locationAread1\", checkLocation(F.col('areaId1'), F.col(\"locationList\")))\n    \n    return df_hydrated\n\n\ndef filter_data(df, params):\n\n    def filter_option(df, option):\n        p = params[option]\n        df = df.where(df[option]==p)\n        return df\n    df = filter_option(df, 'isWeekend')\n    df = filter_option(df, 'isEventDate')\n    df = filter_option(df, 'isPassedLocation')\n    \n    hourGroup_dict={'0-3':0, '4-7':1, '8-11':2,'12-15':3,'16-19':4,'20-23':5}\n    hourGroups = [int(hourGroup_dict[h]) for h in params['hourGroup'].split(',')]\n    if len(hourGroups) <6:\n        df = df.where(df['hourGroup'].isin(hourGroups))\n    \n    return df\n    \n    \n    \n# Create Aggregate data for prediction Task\ndef aggregate_data(data_frame, streaming=False):   \n\n    processed_df = data_frame.withColumn('hourGroup', hour_group(F.col('currentHour')).cast(\"int\"))\n    # Treat Uncertain Data  \n    lower_thresh = 0\n    upper_thresh = 600\n    processed_df = processed_df.where(processed_df.ellapsedTime.between(lower_thresh, upper_thresh)) \n\n    ### Select Relevant Columns ### \n    processed_df = processed_df.select(\"day\", \"areaId1\", \"distanceCovered\", \"ellapsedTime\", \"journeyPatternId\", \"vehicleId\", 'busStop', \"hourGroup\", \"congestion\", \"locationAread1\", \"eventDate\", 'TimestampDay')\n    \n    df_indexed = column_to_index_with_pre_index(processed_df, f\"{base_path}indexer_areaId1\")\n    df_encoded = column_to_ine_hot_with_pre_encoder(df_indexed, f\"{base_path}encoder_areaId1Index\").withColumn('areaIds', sparse_to_array_udf('areaId1IndexVec')).drop(\"areaId1IndexVec\", \"areaId1\")\n    \n    num_area_id = 12\n    ### Groupby & Aggregate ###\n    if streaming:\n        grouped_df = df_encoded.withWatermark(\"TimestampDay\", \"1 days\").groupBy(\"journeyPatternId\", \"vehicleId\", \"TimestampDay\", \"hourGroup\")\\\n                           .agg(F.array(*[F.max(F.col(\"areaIds\")[i]) for i in range(num_area_id)]).alias(\"areaIds\"),\\\n                                F.sum(\"distanceCovered\").alias(\"TotalDistance\"), F.sum(\"ellapsedTime\").alias(\"TotalTime\"),\\\n                                F.max('eventDate').alias(\"isEventDate\"),\\\n                                F.max(\"locationAread1\").alias(\"isPassedLocation\"),\\\n                                (F.sum(F.col(\"congestion\").cast(\"int\"))/F.count(\"congestion\")).alias(\"congestionPercentage\"),\\\n                               F.count(\"*\").alias(\"recordCount\"))\\\n                            .withColumn(\"isWeekend\", date_format(\"TimestampDay\", 'EEE').isin([\"Sat\", \"Sun\"]).cast(\"int\"))\n        \n    else:\n        grouped_df = df_encoded.groupBy(\"journeyPatternId\",\"vehicleId\", \"day\", \"hourGroup\")\\\n                           .agg(F.array(*[F.max(F.col(\"areaIds\")[i]) for i in range(num_area_id)]).alias(\"areaIds\"),\\\n                                F.sum(\"distanceCovered\").alias(\"TotalDistance\"), F.sum(\"ellapsedTime\").alias(\"TotalTime\"),\\\n                                F.max('eventDate').alias(\"isEventDate\"),\\\n                                F.max(\"locationAread1\").alias(\"isPassedLocation\"),\\\n                                (F.sum(F.col(\"congestion\").cast(\"int\"))/F.count(\"congestion\")).alias(\"congestionPercentage\"),\\\n                               F.count(\"*\").alias(\"recordCount\"))\\\n                            .withColumn(\"isWeekend\", date_format(\"day\", 'EEE').isin([\"Sat\", \"Sun\"]).cast(\"int\"))    \n        \n        \n        \n    grouped_df = grouped_df.where(grouped_df['recordCount'] > 20).drop(\"recordCount\")\n        \n    grouped_df = grouped_df.select(\"*\",F.when(F.col(\"congestionPercentage\") > 0, 1.0).otherwise(0.0).alias('isCongestion'))\n        \n    return grouped_df\n\n\n\n# Process data for prediction task  - 1. OneHot Encode journeyPatternId & hourGroup; 2. Assemble as Vector  \ndef encode_and_assemble(data_frame):\n    data_frame_fixed = data_frame.withColumn('isEventDate', F.col('isEventDate').cast('int'))\\\n                                 .withColumn('isPassedLocation', F.col('isPassedLocation').cast('int'))\n    \n    ### OneHot Encode journeyPatternId & hourGroup ###    \n    indexed_JPI = column_to_index_with_pre_index(data_frame_fixed, f\"{base_path}indexer_journeyPatternId\")\n    encoded_JPI = column_to_ine_hot_with_pre_encoder(indexed_JPI, f\"{base_path}encoder_journeyPatternIdIndex\").withColumn('journeyPatternIds', sparse_to_array_udf('journeyPatternIdIndexVec'))\\\n              .drop(\"journeyPatternIdIndexVec\",\"journeyPatternIdIndex\")\n    \n    indexed_HG = column_to_index_with_pre_index(encoded_JPI, f\"{base_path}indexer_hourGroup\")\n    encoded_HG = column_to_ine_hot_with_pre_encoder(indexed_HG, f\"{base_path}encoder_hourGroupIndex\").withColumn('hourGroups', sparse_to_array_udf('hourGroupIndexVec'))\\\n              .drop(\"hourGroupIndexVec\", \"hourGroupIndex\")\n\n    encoded = encoded_HG.withColumn(\"areaIds\", to_vector(\"areaIds\"))\\\n                     .withColumn(\"journeyPatternIds\", to_vector(\"journeyPatternIds\"))\\\n                     .withColumn(\"hourGroups\", to_vector(\"hourGroups\"))\n\n    features_list = [\"areaIds\", \"TotalDistance\", \"isWeekend\", \"journeyPatternIds\", \"hourGroups\", \"TotalTime\", 'isEventDate', 'isPassedLocation'] \n    \n    sizeHint = VectorSizeHint(inputCol=\"journeyPatternIds\", handleInvalid=\"skip\", size=583)    \n    datasetWithSize1 = sizeHint.transform(encoded)\n    sizeHint = VectorSizeHint(inputCol=\"hourGroups\", handleInvalid=\"skip\", size=5)    \n    datasetWithSize2 = sizeHint.transform(datasetWithSize1)\n    sizeHint = VectorSizeHint(inputCol=\"areaIds\", handleInvalid=\"skip\", size=12)    \n    datasetWithSize3 = sizeHint.transform(datasetWithSize2)\n    \n    assembler = VectorAssembler(inputCols=features_list, outputCol=\"features\")\n    \n    input_data = assembler.transform(datasetWithSize3)\n#     input_data = assembler.transform(encoded)\n\n    return input_data\n\ndef train_model(data, maxIter):\n\n    # regression Model\n    lr = LogisticRegression(featuresCol='features', labelCol='isCongestion').setMaxIter(maxIter)\n\n    # Fit and Predict \n    lrModel = lr.fit(data)\n    \n    return lrModel\n\ndef evaluate_model_wrapper(data):\n    predictions = lrModel.transform(data)\n\n    # Create both evaluators\n    evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"isCongestion\", predictionCol=\"prediction\")\n\n    evaluate_model(predictions_train, \"Test\")\n    \n    return predictions\n    \n    \ndef save_indexer(c , batch_df):\n    indexer = StringIndexer(inputCol=c, outputCol= c + \"Index\", handleInvalid='skip')\n    indexer = indexer.fit(batch_df.select(c))\n    indexer.save(f\"{base_path}indexer_{c}\")\n    \ndef save_encoder(c , batch_df_indexed):\n    encoder = OneHotEncoderEstimator(inputCols=[c],outputCols=[c + \"Vec\"])\n    encoder = encoder.fit(batch_df_indexed)\n    encoder.save(f\"{base_path}encoder_{c}\")\n    \ndef raw_data_to_predictions(streaming = True, USE_TAKE=False, min_date='2018-07-01', max_date='2018-09-11', filter_params=None, path_ip=None):\n    lr_model = LogisticRegressionModel.load(f\"{base_path}model_new_new\")\n    df = create_warehouse_df(streaming=streaming, USE_TAKE=USE_TAKE, min_date=min_date, max_date=max_date, path_ip=path_ip)\n    if df=='problem':\n        return 'problem'\n    df = hydrate_data(df)\n    df = aggregate_data(df, streaming=streaming)\n    if filter_params:\n        df = filter_data(df, filter_params)\n    df = encode_and_assemble(df)\n    predictions = lr_model.transform(df)\n    return predictions\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["!pip install libify"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["import libify\nlibify.exporter(globals())"],"metadata":{},"outputs":[],"execution_count":6}],"metadata":{"name":"Lab4_functions","notebookId":1325942436209433},"nbformat":4,"nbformat_minor":0}